{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m9AsVmE3-JP7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "#\n",
        "# Shorthand:\n",
        "#   \"pd_\" as a variable prefix means \"partial derivative\"\n",
        "#   \"d_\" as a variable prefix means \"derivative\"\n",
        "#   \"_wrt_\" is shorthand for \"with respect to\"\n",
        "#   \"w_ho\" and \"w_ih\" are the index of weights from hidden to output layer neurons and input to hidden layer neurons respectively\n",
        "#\n",
        "# Comment references:\n",
        "#\n",
        "# [1] Wikipedia article on Backpropagation\n",
        "#   http://en.wikipedia.org/wiki/Backpropagation#Finding_the_derivative_of_the_error\n",
        "# [2] Neural Networks for Machine Learning course on Coursera by Geoffrey Hinton\n",
        "#   https://class.coursera.org/neuralnets-2012-001/lecture/39\n",
        "# [3] The Back Propagation Algorithm\n",
        "#   https://www4.rgu.ac.uk/files/chapter3%20-%20bp.pdf\n",
        "\n",
        "class NeuralNetwork:\n",
        "    LEARNING_RATE = 0.5\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_weights = None, output_layer_bias = None):\n",
        "        self.num_inputs = num_inputs\n",
        "\n",
        "        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
        "        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
        "\n",
        "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
        "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
        "\n",
        "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
        "        weight_num = 0\n",
        "        for h in range(len(self.hidden_layer.neurons)):\n",
        "            for i in range(self.num_inputs):\n",
        "                if not hidden_layer_weights:\n",
        "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
        "                else:\n",
        "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
        "                weight_num += 1\n",
        "\n",
        "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
        "        weight_num = 0\n",
        "        for o in range(len(self.output_layer.neurons)):\n",
        "            for h in range(len(self.hidden_layer.neurons)):\n",
        "                if not output_layer_weights:\n",
        "                    self.output_layer.neurons[o].weights.append(random.random())\n",
        "                else:\n",
        "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
        "                weight_num += 1\n",
        "\n",
        "    def inspect(self):\n",
        "        print('------')\n",
        "        print('* Inputs: {}'.format(self.num_inputs))\n",
        "        print('------')\n",
        "        print('Hidden Layer')\n",
        "        self.hidden_layer.inspect()\n",
        "        print('------')\n",
        "        print('* Output Layer')\n",
        "        self.output_layer.inspect()\n",
        "        print('------')\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
        "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
        "\n",
        "    # Uses online learning, ie updating the weights after each training case\n",
        "    def train(self, training_inputs, training_outputs):\n",
        "        self.feed_forward(training_inputs)\n",
        "\n",
        "        # 1. Output neuron deltas\n",
        "        pd_errors_wrt_output_neuron_total_net_input = [0] * len(self.output_layer.neurons)\n",
        "        for o in range(len(self.output_layer.neurons)):\n",
        "\n",
        "            # ∂E/∂zⱼ\n",
        "            pd_errors_wrt_output_neuron_total_net_input[o] = self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])\n",
        "\n",
        "        # 2. Hidden neuron deltas\n",
        "        pd_errors_wrt_hidden_neuron_total_net_input = [0] * len(self.hidden_layer.neurons)\n",
        "        for h in range(len(self.hidden_layer.neurons)):\n",
        "\n",
        "            # We need to calculate the derivative of the error with respect to the output of each hidden layer neuron\n",
        "            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
        "            d_error_wrt_hidden_neuron_output = 0\n",
        "            for o in range(len(self.output_layer.neurons)):\n",
        "                d_error_wrt_hidden_neuron_output += pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].weights[h]\n",
        "\n",
        "            # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂\n",
        "            pd_errors_wrt_hidden_neuron_total_net_input[h] = d_error_wrt_hidden_neuron_output * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input()\n",
        "\n",
        "        # 3. Update output neuron weights\n",
        "        for o in range(len(self.output_layer.neurons)):\n",
        "            for w_ho in range(len(self.output_layer.neurons[o].weights)):\n",
        "\n",
        "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
        "                pd_error_wrt_weight = pd_errors_wrt_output_neuron_total_net_input[o] * self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)\n",
        "\n",
        "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
        "                self.output_layer.neurons[o].weights[w_ho] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
        "\n",
        "        # 4. Update hidden neuron weights\n",
        "        for h in range(len(self.hidden_layer.neurons)):\n",
        "            for w_ih in range(len(self.hidden_layer.neurons[h].weights)):\n",
        "\n",
        "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
        "                pd_error_wrt_weight = pd_errors_wrt_hidden_neuron_total_net_input[h] * self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)\n",
        "\n",
        "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
        "                self.hidden_layer.neurons[h].weights[w_ih] -= self.LEARNING_RATE * pd_error_wrt_weight\n",
        "\n",
        "    def calculate_total_error(self, training_sets):\n",
        "        total_error = 0\n",
        "        for t in range(len(training_sets)):\n",
        "            training_inputs, training_outputs = training_sets[t]\n",
        "            self.feed_forward(training_inputs)\n",
        "            for o in range(len(training_outputs)):\n",
        "                total_error += self.output_layer.neurons[o].calculate_error(training_outputs[o])\n",
        "        return total_error\n",
        "\n",
        "class NeuronLayer:\n",
        "    def __init__(self, num_neurons, bias):\n",
        "\n",
        "        # Every neuron in a layer shares the same bias\n",
        "        self.bias = bias if bias else random.random()\n",
        "\n",
        "        self.neurons = []\n",
        "        for i in range(num_neurons):\n",
        "            self.neurons.append(Neuron(self.bias))\n",
        "\n",
        "    def inspect(self):\n",
        "        print('Neurons:', len(self.neurons))\n",
        "        for n in range(len(self.neurons)):\n",
        "            print(' Neuron', n)\n",
        "            for w in range(len(self.neurons[n].weights)):\n",
        "                print('  Weight:', self.neurons[n].weights[w])\n",
        "            print('  Bias:', self.bias)\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        outputs = []\n",
        "        for neuron in self.neurons:\n",
        "            outputs.append(neuron.calculate_output(inputs))\n",
        "        return outputs\n",
        "\n",
        "    def get_outputs(self):\n",
        "        outputs = []\n",
        "        for neuron in self.neurons:\n",
        "            outputs.append(neuron.output)\n",
        "        return outputs\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, bias):\n",
        "        self.bias = bias\n",
        "        self.weights = []\n",
        "\n",
        "    def calculate_output(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = self.squash(self.calculate_total_net_input())\n",
        "        return self.output\n",
        "\n",
        "    def calculate_total_net_input(self):\n",
        "        total = 0\n",
        "        for i in range(len(self.inputs)):\n",
        "            total += self.inputs[i] * self.weights[i]\n",
        "        return total + self.bias\n",
        "\n",
        "    # Apply the logistic function to squash the output of the neuron\n",
        "    # The result is sometimes referred to as 'net' [2] or 'net' [1]\n",
        "    def squash(self, total_net_input):\n",
        "        return 1 / (1 + math.exp(-total_net_input))\n",
        "\n",
        "    # Determine how much the neuron's total input has to change to move closer to the expected output\n",
        "    #\n",
        "    # Now that we have the partial derivative of the error with respect to the output (∂E/∂yⱼ) and\n",
        "    # the derivative of the output with respect to the total net input (dyⱼ/dzⱼ) we can calculate\n",
        "    # the partial derivative of the error with respect to the total net input.\n",
        "    # This value is also known as the delta (δ) [1]\n",
        "    # δ = ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
        "    #\n",
        "    def calculate_pd_error_wrt_total_net_input(self, target_output):\n",
        "        return self.calculate_pd_error_wrt_output(target_output) * self.calculate_pd_total_net_input_wrt_input();\n",
        "\n",
        "    # The error for each neuron is calculated by the Mean Square Error method:\n",
        "    def calculate_error(self, target_output):\n",
        "        return 0.5 * (target_output - self.output) ** 2\n",
        "\n",
        "    # The partial derivate of the error with respect to actual output then is calculated by:\n",
        "    # = 2 * 0.5 * (target output - actual output) ^ (2 - 1) * -1\n",
        "    # = -(target output - actual output)\n",
        "    #\n",
        "    # The Wikipedia article on backpropagation [1] simplifies to the following, but most other learning material does not [2]\n",
        "    # = actual output - target output\n",
        "    #\n",
        "    # Alternative, you can use (target - output), but then need to add it during backpropagation [3]\n",
        "    #\n",
        "    # Note that the actual output of the output neuron is often written as yⱼ and target output as tⱼ so:\n",
        "    # = ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
        "    def calculate_pd_error_wrt_output(self, target_output):\n",
        "        return -(target_output - self.output)\n",
        "\n",
        "    # The total net input into the neuron is squashed using logistic function to calculate the neuron's output:\n",
        "    # yⱼ = φ = 1 / (1 + e^(-zⱼ))\n",
        "    # Note that where ⱼ represents the output of the neurons in whatever layer we're looking at and ᵢ represents the layer below it\n",
        "    #\n",
        "    # The derivative (not partial derivative since there is only one variable) of the output then is:\n",
        "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
        "    def calculate_pd_total_net_input_wrt_input(self):\n",
        "        return self.output * (1 - self.output)\n",
        "\n",
        "    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n",
        "    # = zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n",
        "    #\n",
        "    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n",
        "    # = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ\n",
        "    def calculate_pd_total_net_input_wrt_weight(self, index):\n",
        "        return self.inputs[index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example\n",
        "![Backpropagation.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPMAAADPCAMAAAAXkBfbAAABZVBMVEX////P4vPi4uLZ6tMAAAD8/Pz19fXl5eVtbW3r6+uBgYH5+fnv7+/o6Ojs7Ozy8vLY2NidnZ1PT0+jo6O1tbV4eHg2NjbU6PnNzc3e3t7S0tLHx8fe8NjKysq5ubmioqJdXV1FRUWMjIzE1uZTU1NkZGSXl5dbW1s9PT0vLy+ywtAiIiISEhINDQ19fX0dHR3w9PyPm6aCjZf87++5yLRxeoLrqqrO3sj65eXbaGjXTEygrrvdfX3ut7fYWFhUWmCIk53zy8uls6DMAABfZ23ec3Pom5vwwsL12tqQnYxJUFZzfHDSJyfXU1PK1vJ7mt+ctOdZgNfrqmyAi3xkbGLkj49rcmnVPDzB0Lw+QzxmcHiap5V2gYlMUkqsuqfPFBQradO9z/B2ktuKpeKrvuoAVc1IedbUJSXVgIB3lt0jZNGQreb78OPyyKD33cbqoVrvvIvnlDT44s7lihzpnUr007TwwJR+jfKyAAAZiElEQVR4nO1diVvbxrYfG8mbZMu7LcsYW16QbLABsxjMXrZAQwKBJI/QNCS5uX03pElz2/z9T5JlaSTNyJuI/X2P39c0OEjDnDn7mZkDAI94xCMe8YhHPOIRj3jEIx7xiEf8/wXJeIZ+1xP0hkgH56IilKn48lFfIOd1fmzg5wKlqARffPDRSXel/vLs7GUikKMdnVSoUhBWFtvVxaVToZxxdGgAmIhQuG1XJSzuCInwYC9z+Rdvp7Iy1vavAg5SHWncVFMqiruNsqMLmhN35MFdLpc8+u0cP8C7Id+TbHZKRTb7+irm0KTo6GlRnpKKlOu2kXNoaAkJsZ2CRy/uiP5+301fvdYo7pD9JjDQD6cw/+4Vb11GpNq1HuvZXAXk0XVT/wduZoZBPpnYgddTwa7Yp1ZnXq5NmZB9iyd6rwlaW1sn0L/EIugnmdKSeVKuVLUxbTub9Q1wdLS1Ln1F0SAIPKFSOINc08qpeWxp9KVSX8rD/LaWNdM8ld3H6cbWLy2wed2heQbk3EwuNouhOXBgnZXEacFOALc218F2k1yX/AfrA4SnEq8FZlAPTtcsXJZx0JeE+ixcVog+Q9vY5uphCzzdvFY+lBghFo8DdwX5aDpZREzKlbqxmVZro7kONiSaJeEmfSzBJdxxT5mzPkiVFlEku4rJPnwD98LKZQUFzAubLfLo5OhQ/jIWqwd8FGDRNBfQs5KmhXcqWxuvflk9bFLb8oeIkC7w/jCYiVsfjJ+iB08tlnqSTPrQFE9ln2As7GaLOgEtZVJeYto3C0AYSXOmgGSzNK1bHKOb0qjSyMcbh4ocsQSYmw41YlGEMpTaaJptV1QFi2Pz1JTP+vTeqsLnjdX1VeWzj5ZEG7gRfACAtxowFdUCJlYkW50/JydKOEh6QZAE/jTCLIVxC+pKrWCsi46ZtziSs1fWmbUk03VCgebWca9xARCrmFm5iqfp3q/bI35gWNBUUfthqXZP4a7rFuzJm+zU2WtIuAeM5YygCH0WS1LokFrc7U4zdYMUjEFQ14VIifAEfQ2KRI93yYQu2m/2X9eIZT0e2x9pZm5Rm9TiabudOiBuNZpveUD2BeBBg4xq6lzcIU5donCj851gtOeQjp0+02k8e7H85H8gmpejCd/wmD3VaZ7fma8u3axoNC/W+h05ikaJ6NKcWioUT5dcu5Cs14TuYwWkcQ0ZaM6uwTS/JmYDQ8NHFHSad1yni6lbiGaiwPc1Ch9DIiLqNK/cpFZWUksQzY1K97kZhGOXUuaXWJqnlpPR4bMgnyBANKd2DDQvlQl26JElVMRo1/en2sTKfNtAcy99BnXdZr08y66JEJ/f5PLRYWcVaLC6DVs8TUl8XrnRbVguRoxQQIgROV43Du1biedV3V1XxV7v88s60RK5kLPOvmCYJMJH94MIkQbJqm5aXQbPIvuqwFxouKElPzUfY6Z3tNHUBFr7uFvvNQD3BheTZGdJ4CaQ8X0vTBNcCOisMKNaoGThH7IMxTUiQcpfwHr/nZ4ZOn2FoTn7Ws6sOAJpBuzhJWI0DcICLvZckUemorODjyyBrQXooJQ846K8qti7IMgvo4nOXimZeoUYODKhxATwSpzEZD6uoqAYMHqgUk4XtFD2+KWZZdBJm2Qr+hjV8xuS5uyy+m5dHNR4z0YBLSsrZlqpboobJAYvQXnygofyyloRuEGtaKqd7Ge6OWSWsZZXYxgqX8KVftAI1BiFzZII7aBmtSh0Sz1hYtDiGJUQvSCkUMUkEWKUKor96WJgH0H0S02k6WRPQwgjJrleJqh86SmvIBgxpytLnBgw2eDFtJRtdXgQbljSyVQx2TOpUhGw2u6XULXBPYgMpmWjF1Q56S+sGL2UXAOEeVuZc/c/tLSec9LLdDeb5sRFE8lVAV29QKFyZnDM2eXfDGESR9iX7SAw8vp0FE6Gp1SoQuXYlGulZpS9RMPisZpNQLVauj55tWWZVhbfrX0rIx4UU1AQsNvol8sywqUna1mlxC39f/nKXMeIwMa71ewk9jqCmnegRDmI8UNmJDa3o9f0l+Z95khk1lyCon49BtuHm1pdNV72qcwLE/IXqt50nuWJm6qrM3h1tzE7YEDL1a+e7L9dfrv/wlexRkgBvVh+vb692tzY3Na/6UvMdtUyL1NAeWGjR1cKjYOldnvpVhBRuys1U6h3/eoY/Lq1J/9QfzzmSZQzXGc+bDIh/xU0uN8QH60VDm5vdsRkYohCRIiNRyoxzouy0lR+tmvGj0DrcGsVPJWr7gEwEw5VYmBa9Wu8sjR00Pgyw0bq5XKC55BeJNQwONSt69Vj8unW0y0AZriEj+OjfELhc6iUl60EZTYAHm8uxs/Ew87usskI1nR539462gKbMifKocJMLAeY2Y7FyxHy32RwsJ3RDAHXJl4dbjyVNHpvU5LlBB9IpPOALEs6T/kKSlbid5q2c2nwhYUF1LfY7szI7WtwdAw2ZY2OVQKBMmBKOfUR5W8miHrfBpzusfZOWq3ro+bvrc1VicxkLJYkfbmcLNOBmqIXunm0w4JLniiSDjPefXi3cPfu2XPMzGSLe9zcPjxp7W03f5dlO0RkfHlQi7llMv1Ex2oG0btLNogTXYE93pPplrislFU5r5cDJM9LajWjug66n02480/v34OFCzQdRrieged3nz/cYcKuSkNa6evW9ebhKjg67BQ+eYbj6HogIS0HVegYI09fnDAiQvQgJa5GbCTS2pjxzAUuwGcM74y4+wA+Pv/84dl7zPcTgp0uJZKdv0PDKJyvYPvWdEN1V/3pzYULvF8A5/3QfC7RfLcAqAuMIlClPJ6FlUaHVf1xwoJC2UY6wsmuAfX2ZR7fp8AF1Q/NC3fkv1Kfzi/u7i5wj9hkf7muHepL4azwNBLY74Wk9FF9qh+9OU99fHb+DvRF80dw/v4OLDx7hrd3bgJT8nZ3MySyP05YQRO49SQL2p5yX+bx7hw8l2lInQ83EzMwZROa6Aa6nqFLeyxmPZX0Uf3S/QCnmnojUkPEtNRsvvvl4I5KA4dMLMnKvBawhoYuG44GX8FKVV3s2q3+QgYMIgQisVTSxw5I91DmcXRQBUvprqLXqoe0YCoChGU9c1DuPnCA5xj8DVOmmdMTTWo4R6UhIZrEJEPoRYChzaMDMBkb+ONobFYiAMNntxjQF3F48+gA4rDx9kCp4KAZlRWMIQJQ00cVI5hHB8DXNGND5iH19gRH9iVw/VfyUnBNZBTz6AB8QpehPAFpsBOcSOv130AD9ouOJ84DgirkO19E4O1FZzih1X8rDdhfDxnHOwh/R4tzhrBsqIzKCnUd4/OGgv+o5tEBhGVrHTJYcMc4EWh4ZBk37oiOKx6BESfCpGjIhZwLGaJR4K4Zo4CxOioNfC1qKE2P7qh0iGVzPu3g4KOgYDwT4iQnWEI0phOWCu94EG8UDNV4B30JmRAaxi2ycTuqDjIEF4KC4ZFDbRh8LZM2eAQnBx8etGxW4f1jB32JEtvG4cRyAhyVJH0dk63vHzsYMuQ6HrBCaH5g/PGIjLJqsvmaarkYxziRrqkak0h2Q9nxJc4Q+FrXpvjUo9iO5baskFCZSmon8saYOGuIE9opBLpjvIfZvECCKejpo0cYfmfEabDw0ZegEiP6HcptyTJ8KcfbKacGx++oGOPxEnkFnMptycC8IbJRajBjqvDC8BRMBwMk4+2UBYvMm8q9cj19XBVeCN29OB28mHbGl8QblrJ+jMgEx+6oKnMWnpKzBUemxc0hjv2oe+3jBIc6/ukWhzzybByF4FGKm7fI1U8GclOJot1E/2fRcAiKPpS0eILWTYSfClqm7XhzD7RWV/e6/5jhc3DlbtihZ6NIfxdkyLnBLiY7CyoqyfDJ09ZG6+h67wSQNGBIko5yeRbE5ke7tk8lkoYUuSvlVJAE3jmnrqAPgYB8ULHZAtsnh+ubTeCJAiGci3mn89J068mR/BVf63qprfX14+bhtnpRL+TlfYnoyFI0NCJEx1NuHpKrJ1vbUrAdrMUCQXqmJNu1PO4ebT+I6eny+gkAR6vkvyV7SbnDXo4E+ekc4XTzhz7RTRwPrwF5DJrbFIhHc2UfHQecbME8w95XAVr6qODfT38/OTwB600QyAR8XABkyvLPHktq1TlXsXd9/MvmYetwc/1IvhLpmY2ExFhJ4YJ3iGP3HaTnIbPfAnuHHZq5SL1cmQY+WYr4+TEE3VRn96zZap6c7FHUnmK2wyDoDwY5tRbIDal26uFVBc3mNbm3ebRFvZJzNp4XGEpQvjE7+9MTSnIWc8MW3ryIDH5fRR6hpJND/g4On75qNV9tSGIEfPG4T3JiyncoR+KegRCooZMbY3Uu0MDkA95MJoMRTjJfwFp8OMli5tFuOhznfaUAnxtK4dncDB+oxJH5wgzuqp+xOkfmo9a36Vy+liwUCqLo4xAL5xOxJ82NFd4gYQ3Hmbgo3CwttneXDsTSoDfBmNjLF/uvJSy/eMlbJoE+0gOsmxd00ny0zVMRdhaLCqpLp1GLxkfwbsiclU9bDmrlCjvVYuecvqu4eJofSLVyV2/XOt13pP8vnwVMGydYi8yYd9nNlw3D0Z2qdg0l5WoLCaMgx23snqXCGzfaC7IutKErLqniktj/LXWq/iJrvIMShVlNmk/N6LDushtPzXHEItwRSJrhjaE7DjeHT04QFV5+Hl6Gsrn3Tqo61y/RVP6t6a5Rduo36KZNCXsjGFUTgo03R1QtV6AW5/XAOtywueiHqvAmGrou+XasjX2K830SnUfdmLzS1IyvYSsCyG0k/b5KWLSQLN/VjXbn7U0mbIoNyApvudBd5ZhgGVrmtNiXTkdQ1+iya1fq4DG800XvL1DlUudVanYXeaVxR2UuU8rblLrQFV6moBpJt4BYT3lF870pBuxL9B3Rt52ZZWyMDGZ/wV/rvBpHNH6Sod6LpepJu+1bzIGcrscKrGDuAp/24bISyBZL8l1gmYmMTRCNPZinHnlOYtrjpFYUXtnfMiZxFd5OqcYvYO5899NvKHiGu+e+HJEjPpv7ofhd9mlZODgMm2VG++Uwx5Yh+AM5Slhv6kMDD57suT8f03vvZKeM/QzykskQbF61OQ4WaYRBQL+JLt+DTkGXoXc4ELd3K3ZbkbKFKRsvAkuDa58PeppuqPfO2+Wpt2+g3jtnHr5mY2RsNy8SSUYT7dTijRQo7RROu+KYuuXDjYrt7oTtViQ/54GaGclR2I2g9TdILfUqn1F6753skzdv5q7mdJrf8A07jaPtMlpqttzoMja1WLhdKorF064ZT7VLQo+J2RfyfQVRl5r2wWJRqO7caoPn7YeG1Tn74sny6zVCp3mfqLjDWGQ4Lo5HLkZA/UnmluarqareO6ZNJHMcl4tgUeGhD3zCjAAxpy1om1iSBi7O6YPX6tqDSDGH2ocpfWgakEl7S9hjPmkDIVnTaT5InbarhKbf0jwFsVYTMc2E5EZB8Ie8heZEuabRfLOycLtSnNeb3LRr2mM+JM1uQ7+hNRHq3JHdj5Gonk6U/IfysCGKpBSQ6t8QSJL2QnyWe+/MCW1N5doF0HkKA4/X8D3LrGlWk+3UrlgVlpKiPvhiL9lmoH5DL1+cEcmkZtOyb9C320nlD2Wfo1Ne0NBJXHGt3EoJtNYjaLdHzcPgqKyazXhBTbNhxRvxpioN3pWi3jYM6jc0tfx6eX9/X+fzC5TRbh5ub4Hj7XWv7qjC1kSBkr49q7kTuWNaJ8ntyiPenRytr5+0ttc3tZH4WXNUJEkB7Ku6t+i7n3v7KmNLliyUU2ZnUbbzeBX8Qm03t/6QfjYHOA+dAYKFa6RXYlQM2UBEYU0BX65+1QRgSy6GATcLciCcCwBTYxlKDv9GiknCT3Bx2BpaRqin6611IP0HKAEQ0zNcnbM8GJQzq6CI673TxrerIn/Z/rX1x6+vruUr3yGCLIeT5bzxCUXCRoo9KWyvyzNMnaq1sbcOTuT+ngGuXKlz0VzeVNAIdaon2JZApzY14RbY+oOlmnLrgEQ8H6/neLoEmw5SPV1SHyXHyGEYvVZGrRC1tQW2T9abq3IDzjSRKSXYCp80VssYNUBjk+h0b7GEK1WTzda1RPMfLYXmStJN5GIxAHd+IP3q+uJzyX5On+TRidUVSi22rpuv1tfB6qtXSuNkgkzITVLMR627dgDVRVgKjeewDSaar8D29kZ479cN+Z57JgrmaTrqgxefDjpSM6B/Q0n2E2wOKTdUsHFUlLtH+aZXM2q7mJaGYnzk4P0WxNirKbN425Asw+ZoMQk5MSnqtpbpenUEsktdGEO2lbcO3uh7yyz80lgRy06d2b5rc7SYNB5gIxNC29B7Z7dnEySbM7ywCKmDG2q9u+IAu4TBxBO9B3d26u2Vve2zObEVMkt9vHDQdnXL7u3TUq+NZJvEmfKaE/a4YKjplwbrRDOdP5ObIUohydqTfMz+6JPNtGhr6YSJqdsrtwe1aK6nUcUnzmTQutR0pCbc7C62F3cPaoXBd0XdsUT+6irvq/TsIYWfFoPWxXCcr5frlVw/py/wZ3iD6NfTscBsqczHh72xgUhhUD8cNy3P6GfLKWyNzaI1zkCmmOpJNtawWvVtcGBrbPTo90+ReOYC5xe47iQacI6KdIBk7BnegURo4U6i4byfthXk+3+5wMXC+f/2mBaGzSRG3wYCziNQg1wdJN99/OR6/v5DD0KUR8nn5wsXYOHCXohw/tMJfcN5hMG05vwZuHv+CYCeEivjs0Izrt2Q/bQc0TeMRyAHO6m/8J+7Tx8+P3v+n140y13DPp+Di9Tds2GmxThxHQhz5RLlmG1x9/Hz84WPH3sIrCIQ4LkL3H36lLJ9Dm1YTVHhkMBsBoUGEqGFz+Dd+YXrw8fnfehzX0BbMCdMNsCtJz3YZQ+5b5IksAvPPzp1vh/pqJTy1+hArycz7isoyIxKq12MCOR6OhDbjQgaZU0G0zcskO2EnIjtRgPSUTlisgHa8TukNaMAZVg9TnVXQC3doF7qAYAwrANFhXZArad//BdiEYbVOX1DrOdD5VKDwJoBDBwiYYFYT6cMxShAOCqHvBRAOarxeymAMqzOCZ+VzdQk3PS2HgfDlL+GQci8nqR73I5ZhuXcsmNeCuH4B0wfHwpmw+pkiGTOUJ0KZ0eEWeMc5YR5+RwKZ0eF+TCiE+WvLszxiIOGYhSYNc7REMkkMhPhpYBF4xzlhKnCOxFeClhKVc5ywuj4x58+qjBqnLOcMKqNc+HsqDA4Koc5YVQbJ23jSKBg3+FwIm+M7ybESwFTRuWw8HlgNg9Y5HxAGDTO6awWVptJ8VLAaFidjhfg+G5SvBQwapzjnIAS54nxUsDgqBznBHSnaEJyqQ50jXOeE1BLqolxzADWOOfjBcg6To6XArAFc/4ci0c77jM5XgrArECc/hp1bE1tJqHIqUMLDR9gWpraeCbHSwEoo3qIeKHrqCbJSwH9FwM8xLS6FfMJ2IozQGXFg0yrax0nyUsBTeMeJKvt7jhPwFacAWpG9SCnLVXrOAlbcTBUR/Uw0+rY6snyUqDLipGn5aFpj3UfV4lHxpc+kt5wJp1OZ1iT2iqOarTTXx53hpuWxubSrNEM+pmRxx4BTGY67FbAZjgWWnflOPVIp7884ekMq44dTkPNpUilGUeIVUm+v5d+2v3wP2dQhNPqrBSE05q94pTQMKeEnOSXLxT488uXweblTofdhrHZrojnItLaVsrqBY0fl1+/33+9/OEMQT1BGaclIa3ey6Ly6RAgZ+YVef/248sl+OuffwaimTWspoxMRhWjXDTBUXWaVRpj3H+9//7925/k3z/HgpOceVrSxDpEk4WokHHHEgrN9/c/LsF/v/0zyNhs2jK0RHSHrlw0KLAM8Ck3R77/9fdf91+/g8ufI90WTkjwZjqdYvI0UwqCQCda+n757f7b/eUARLsRJGsLGouDSo5Sf4f390vpv6/3P4nmsFmwO0SnZWNDl0mmRKk0/wPIr39+B1++9T00g1pOWXVke0HFEh7BXeb9shB9p/6+//Ljy4/vfz0MkeZ5IaclTQzIfZ8q5TgAdYnmH99/XH67vP/vl7+/9z02ejll3yCptJ/zlWOMzyf/APIr+Ofy8p66vPzz4SjVkUGzQpJAP6CCmbisbXJUfO8h//yHAvcDmDDscsqMljIqekzpIzWNmxebAf7QSL+Fi8WwWRk75B9bLuWG5+XNwB+mKe9o22UGbTaM7c54Rhx7FITheQUEL/QpzHpHSixIgwjVC0GYZtbrH1suBamc11crwDSz6dGa93uMY0cNNIfHWOSEecHFDXxmp0fLd2jYhHExA5/D4TGmjxw0kaCJ5hF/X4GB5mDcKNvj/LVjMJ+98aSBZmxL2f4Ay7Y0tmCgeZzFIIOl5uoGGzZiAd9gw7y5ADx2epxlTtbgT2BWuHF9lPtGBjs269DvTRkOQWzc4O553b0X3Bnc0OExdcdXgY/DRv6VK/gYLz3ecjYuQPSmR4+GWQyjw5nx1nYpTCIwOpvlsdEJzPS4N2q8GS9qXo5YGT+S6PT4fycoKsv1OsQKN0KlM2P/3UwSpODXPC/OqZghaC62selJIFku9RqZzDoYMjAZWL7ZTHpStuNC6YyBE05GDKQ3kw7LoQ/LhtNpdnK2pshgWp6YNC1pgk6XbEiazSgI+yeHYgWeoEQx6w49jOskKc+E0fuIRzziEY94xCMe8Yifh/8DZDmxSl/Ekl8AAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "jYUV47vZWS_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#num_inputs=2, num_hidden=2, num_outputs=2\n",
        "nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\n",
        "nn.train([0.05, 0.1], [0.01, 0.99])\n",
        "print(\"Error rate in the first step is:\", round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3quoWpIP9SV",
        "outputId": "3e353f9f-9dd1-4436-e340-e3c6ca40da40"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error rate in the first step is: 0.291027774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#num_inputs=2, num_hidden=2, num_outputs=2\n",
        "nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\n",
        "for i in range(100):\n",
        "    nn.train([0.05, 0.1], [0.01, 0.99])\n",
        "    print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9))\n",
        "\n"
      ],
      "metadata": {
        "id": "6tubLnnG-ixh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR example:\n",
        "training_sets = [\n",
        "     [[0, 0], [0]],\n",
        "     [[0, 1], [1]],\n",
        "     [[1, 0], [1]],\n",
        "     [[1, 1], [0]]\n",
        " ]\n",
        "\n",
        "nn = NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1]))\n",
        "for i in range(100):\n",
        "     training_inputs, training_outputs = random.choice(training_sets)\n",
        "     nn.train(training_inputs, training_outputs)\n",
        "     print(i, nn.calculate_total_error(training_sets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ABdSgS8JA98",
        "outputId": "0ee6119c-78a6-47fc-ea1e-b3ec6d81ddff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.9024485157909845\n",
            "1 0.9026541103665022\n",
            "2 0.8983791442477991\n",
            "3 0.8930047081763589\n",
            "4 0.8877928146067418\n",
            "5 0.8881863248700127\n",
            "6 0.8885756818623527\n",
            "7 0.8888897105982564\n",
            "8 0.8892717778084074\n",
            "9 0.8830624105837055\n",
            "10 0.8767978933064623\n",
            "11 0.8772294638850805\n",
            "12 0.8703059505772712\n",
            "13 0.8708132657083258\n",
            "14 0.8631331770682499\n",
            "15 0.8638425912337091\n",
            "16 0.8644336114033492\n",
            "17 0.865123333481439\n",
            "18 0.8565557276518196\n",
            "19 0.8470891766746647\n",
            "20 0.8479381509878755\n",
            "21 0.8373222352791883\n",
            "22 0.8258548604776152\n",
            "23 0.8131538149701206\n",
            "24 0.7990825900472395\n",
            "25 0.80123090548527\n",
            "26 0.7858789738284362\n",
            "27 0.7689076146542984\n",
            "28 0.7720804244004029\n",
            "29 0.7536947330448918\n",
            "30 0.7572933529666818\n",
            "31 0.7607400264780682\n",
            "32 0.7642301715188106\n",
            "33 0.7410702668480222\n",
            "34 0.7453912772416559\n",
            "35 0.7244697244552689\n",
            "36 0.7295632702393537\n",
            "37 0.7344116844160642\n",
            "38 0.7390322802580161\n",
            "39 0.7175393747280514\n",
            "40 0.6878027060700581\n",
            "41 0.6548343483853534\n",
            "42 0.6637315287587566\n",
            "43 0.6290300837215889\n",
            "44 0.6398412487324049\n",
            "45 0.6499332298652664\n",
            "46 0.6237083944166062\n",
            "47 0.5976724123591237\n",
            "48 0.5643441204831703\n",
            "49 0.5367089943102608\n",
            "50 0.5216422264678648\n",
            "51 0.5327459138556365\n",
            "52 0.5188443584003427\n",
            "53 0.5305907234984704\n",
            "54 0.5429357956336462\n",
            "55 0.5572759411553794\n",
            "56 0.5374542721331154\n",
            "57 0.5221124949993221\n",
            "58 0.5102300330592162\n",
            "59 0.5173535911275042\n",
            "60 0.5095776933198356\n",
            "61 0.5162650610577542\n",
            "62 0.5273577716138298\n",
            "63 0.5125895553121746\n",
            "64 0.5075245686783478\n",
            "65 0.5107335795569733\n",
            "66 0.5192794005189144\n",
            "67 0.5091417595562197\n",
            "68 0.5080842948134032\n",
            "69 0.5126770458672093\n",
            "70 0.5213830384308725\n",
            "71 0.5332103717128992\n",
            "72 0.5165184967146301\n",
            "73 0.5269142883595848\n",
            "74 0.5399182392572625\n",
            "75 0.5533731098656758\n",
            "76 0.5314880752479929\n",
            "77 0.5446344663019363\n",
            "78 0.5597144901240196\n",
            "79 0.5363381800485152\n",
            "80 0.5506555836609398\n",
            "81 0.5640618115147723\n",
            "82 0.5382087201050632\n",
            "83 0.5207331885318729\n",
            "84 0.5097780361999706\n",
            "85 0.5180439929265622\n",
            "86 0.5289867887633176\n",
            "87 0.5148856823144067\n",
            "88 0.5078889101002564\n",
            "89 0.5076352005728556\n",
            "90 0.5136302798695471\n",
            "91 0.5076817192172406\n",
            "92 0.5138532924903956\n",
            "93 0.5242352146162174\n",
            "94 0.5357650766392694\n",
            "95 0.5170496838842665\n",
            "96 0.5284520598176211\n",
            "97 0.5405015371492233\n",
            "98 0.5533828183458449\n",
            "99 0.5677563439374289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AR3GNHRD-iuh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}